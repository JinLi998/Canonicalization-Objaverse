<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="icon" href="./ICCV_2021_Workshop/oo3d_logo.ico">
<title>OmniObject3D Challenge @ ICCV 2023 </title>

  <link rel="stylesheet" href="./ICCV_2019_Workshop/bootstrap.min.css">
  <link href="./ICCV_2021_Workshop/css" rel="stylesheet" type="text/css">
  <link href="./ICCV_2021_Workshop/style.css" rel="stylesheet" type="text/css">
</head>

<body>

  
<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <br>
      <br>
      <td width="1200" align="center" valign="middle" style='font-size: 24pt;'>OmniObject3D Challenge @ ICCV 2023
          <!-- <span class="title" style='font-size: 24pt;'>test</span></td> -->
    </tr>
    <tr>
        <td colspan="3" align="center">
        <h3>Hosted by <a href="https://ai3dcc.github.io/">AI for 3D Content Creation</a>  Workshop </h3> 
        <h4>The <a href="https://codalab.lisn.upsaclay.fr/competitions/14960">submission portal</a> is now open until 23:59 UTC, September 15, 2023!</h4>
        <!-- <br> -->
        <!-- <br> -->
        <!-- [<a href="https://youtu.be/0_emxwlL6BY">Youtube Live Streaming</a>] -->
        <!-- <br>
        <br> -->
        <!-- 05:00-09:00 PDT, Oct. 17th, 2021 -->
    	<!-- </td> -->
  </tbody></table>
  <br>
  <p><img src="./ICCV_2021_Workshop/teaser2023.jpeg" width="1100" align="center" valign="middle"></p>
</div>

<br>

<div class="container">
  <h2>Overview</h2>
  <p>
    Realistic 3D object modelling, especially from limited observations or random conditions, poses a significant challenge with broad applications in various vision and robotics tasks. To advance research in this field, we present the <a href="https://omniobject3d.github.io/">OmniObject3D dataset</a>, a comprehensive collection of high-quality, real-scanned 3D objects with an extensive vocabulary.
    <br><br>
    In this challenge, we emphasize two fundamental problems: sparse-view reconstruction, involving the prediction of novel view images and 3D mesh from a limited set of input images, and 3D object generation, both conditionally and unconditionally.
    <br><br>
    In addition to the public training set, we have included a hidden test set specifically for the sparse-view reconstruction track. For both tracks, participants must submit their final prediction files based on the provided examples in our codebase. These submissions will be thoroughly evaluated to determine the winners.
  </p>
</div>
<br>

<div class="container">
  <h2>Submission</h2>
  <p>
    <p>Users can participate in&nbsp;<strong>one or both</strong>&nbsp;of the following tracks:</p>
    <h3>Track-1 | Sparse-view Reconstruction</h4>
    <p>This phase evaluates algorithms for novel view synthesis and surface reconstruction given a few posed images of each object.&nbsp;The number of input images will be 1, 2, and 3, as provided in the test set <a href="https://drive.google.com/file/d/1GKEa-r1__tnVKAZSF5I5uWcLh1cAllqh/view?usp=drive_link">here</a>.&nbsp;Submit the predicted novel view images and extracted point clouds in a .zip file. Please refer to the&nbsp;tools and provided examples <a href="https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/sparse_view_reconstruction">here</a>&nbsp;and carefully check the format to ensure a successful submission.</p>
    <h3>Track-2 | 3D Object Generation</h4>
    <p>This phase evaluates algorithms for realistic 3D object generation on the OmniObject3D dataset. Submit the post-processed results on the generated objects in a .zip files.&nbsp;Please refer to the&nbsp;tools and provided examples <a href="https://github.com/omniobject3d/OmniObject3D/tree/main/benchmarks/3d_generation">here</a>&nbsp;and carefully check the format to ensure a successful submission.</p>
  </p>
</div>

<br>

<div class="container">
<h2>Evaluation</h2>
<h3>Track-1 | Sparse-View Reconstruction</h3>
<p>We evaluate the novel view synthesis and 3D reconstruction accuracy by computing the PSNR on 10 test views and Chamfer Distance (CD) between the reconstruction and the ground truth, respectively. PSNR would be the main metric for the Top-5 selection, while CD (under the standard point cloud sampling resolution provided in our codebase) would also be an important factor for the determination of the final award winners. The results are averaged across the whole test set for overall evaluation criteria.</p>
<h3>Track-2 | 3D Object Generation</h3>
<p>During the public submission stage, we evaluate the generation performance by computing the FID score, which would be the main metric for the Top-10 selection. For the determination of the final award winners, user study after the private submission would be the most important factor, and we would also take quantitative evaluation, including&nbsp;FID, Cov, and MMD, into consideration.&nbsp;</p>
</div>

<br>

<div class="container">
  <h2>Awards</h2>
  
  <p>
    The &nbsp;<strong>Top-3 teams from each track&nbsp;</strong> will emerge as the ultimate award winners. They will receive timely notifications just before the workshop and will be officially announced during the event. Winning teams will be awarded certificates of recognition, and they will share in a bonus pool consisting of thousands of dollars in rewards.
    <br>
    Furthermore, the &nbsp;<strong>Champion teams&nbsp;</strong> will be invited to deliver an oral presentation during the workshop, which provides a platform to showcase their outstanding work and share valuable insights with the participants.
  </p>

  </div>

<br>

<div class="container">
  <h2>Timeline </h2>

  <div class="schedule">

  <p><span class="announce_date">Aug. 01, 2023 </span>. Submission start date. </p>
  <p><span class="announce_date">Sep. 08, 2023 </span>. Track-2 public submission deadline. </p>
  <p><span class="announce_date">Sep. 12, 2023 </span>. Track-2 private submission deadline.  </p>
  <p><span class="announce_date">Sep. 15, 2023 </span>. Track-1 public submission deadline. </p>
  <p><span class="announce_date">Sep. 22, 2023 </span>. Technical report, source code, and pre-trained model deadline. </p>
  <p><span class="announce_date">Oct. 02, 2023 </span>. Awards at ICCV Workshop. </p>

  </div>

<h3>Reminders</h3>
<p>1. Public submission for Track-2 (3D object generation) is closed by Sep. 08; during the period of private submission (Sep. 08 - 12),&nbsp; the <strong>Top-10</strong> of participants in this track will be notified and are required to&nbsp;<strong>submit the raw data of rendered images and 3D objects before post-processing, which were used during the public submission period,&nbsp;</strong> to us for a comprehensive&nbsp;<strong>user study</strong>. The results of user study will serve as the most critical metric for the determination of winners.</p>
<p>2. By Sep. 22, the <strong>Top-5 participants in Track-1</strong> and <strong>Top-10 participants in Track-2</strong>&nbsp;are required to submit a short technical report (2-4&nbsp;pages of ICCV two-column template that briefly introduces their technical contributions and analysis of the results), source code, pre-trained model of their methods. The code would&nbsp;<strong>ONLY</strong>&nbsp;be used for verifying the legality of the algorithm and would&nbsp;<strong>NOT</strong>&nbsp;be further distributed. The legality of their method and its reproducibility will also be crucial factors in determining the winners.</p>

Please visit our <a href="https://codalab.lisn.upsaclay.fr/competitions/14960">CodaLab page</a> for more details.
</div>
<br>

<div class="container">
  <h2>Organizers</h2>

    <br>

    <div>
      
      <div class="organizer">
        <a href="https://wutong16.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/tong_wu.png" id="tong_wu"></div>
        <div>Tong Wu<br>CUHK</div>
      </div>

      <div class="organizer">
        <a href="https://liuziwei7.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/ziwei_liu.jpg" id="ziwei_liu"></div>
        <div>Ziwei Liu<br>NTU</div>
        </a>
      </div>
      
      <div class="organizer">
        <a href="https://scholar.google.com/citations?user=oWXEaQoAAAAJ&hl=zh-CN">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/mengchen_zhang.jpeg" id="mengchen_zhang"></div>
        <div>Mengchen Zhang<br>ZJU&SHLab</div>
        </a>
      </div>

      <div class="organizer">
        <a href="https://fuxiao0719.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/xiao_fu.png" id="xiao_fu"></div>
        <div>Xiao Fu<br>CUHK</div>
        </a>
      </div>

      <div class="organizer">
        <a href="https://ziangcao0312.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/ziang_cao.jpeg" id="ziang_cao"></div>
        <div>Ziang Cao<br>NTU</div>
        </a>
      </div>

      <br>
      <br>
      <br>

      <div class="organizer">
        <a href="https://hongfz16.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/fangzhou_hong.jpeg" id="fangzhou_hong"></div>
        <div>Fangzhou Hong<br>NTU</div>
        </a>
      </div>

      <div class="organizer">
        <a href="https://frozenburning.github.io/">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/zhaoxi_chen.jpeg" id="zhaoxi_chen"></div>
        <div>Zhaoxi Chen<br>NTU</div>
        </a>
      </div>

      <div class="organizer">
        <a href="https://scholar.google.com/citations?user=lSDISOcAAAAJ&hl=zh-CN">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/liang_pan.jpeg" id="linag_pan"></div>
        <div>Liang Pan<br>NTU</div>
        </a>
      </div>

      <div class="organizer">
        <a href="https://scholar.google.com/citations?user=GMzzRRUAAAAJ&hl=zh-CN">
        <div class="speakerphoto"><img src="./ICCV_2021_Workshop/organizer/dahua_lin.png" id="dahua_lin"></div>
        <div>Dahua Lin<br>CUHK</div>
        </a>
      </div>
      
        </a>
      </div>


    <br>
    <br>
</div>
<br>

<div class="container">
      <h2>Organization institution</h2>
      <div class="committee_steer">
        <a href="https://www.shlab.org.cn/">
        <div class="previous_small"><img src="./ICCV_2021_Workshop/shlab.png" id="shanghai_ai_lab"></div>
        <div>Shanghai AI Lab</div>

        </a>
      </div>
</div>

<br>

</body></html>
